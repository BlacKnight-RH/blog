---
layout: post
status: publish
published: true
title: Page Cache, the Affair Between Memory and Files
author: Gustavo Duarte
author_login: gduarte
author_email: gustavo-web@duartes.org
author_url: http://duartes.org/gustavo/blog
wordpress_id: 369
wordpress_url: http://duartes.org/gustavo/blog/?p=369
date: 2009-02-10 23:20:18.000000000 -07:00
comments: false
categories:
- Linux
- Software Illustrated
- Internals
tags: []
---
 <p>Previously we looked at how the kernel <a href="http://duartes.org/gustavo/blog/post/how-the-kernel-manages-your-memory">manages virtual memory</a> for a user process, but files and I/O were left out. This post covers the important and often misunderstood relationship between files and memory and its consequences for performance.</p> <p>Two serious problems must be solved by the OS when it comes to files. The first one is the mind-blowing slowness of hard drives, and <a href="http://duartes.org/gustavo/blog/post/what-your-computer-does-while-you-wait">disk seeks in particular</a>, relative to memory. The second is the need to load file contents in physical memory once and <em>share</em> the contents among programs. If you use <a href="http://technet.microsoft.com/en-us/sysinternals/bb896653.aspx">Process Explorer</a> to poke at Windows processes, you'll see there are ~15MB worth of common DLLs loaded in every process. My Windows box right now is running 100 processes, so without sharing I'd be using up to ~1.5 GB of physical RAM <em>just for common DLLs</em>. No good. Likewise, nearly all Linux programs need ld.so and libc, plus other common libraries.</p> <p>Happily, both problems can be dealt with in one shot: the <strong>page cache</strong>, where the kernel stores page-sized chunks of files. To illustrate the page cache, I'll conjure a Linux program named <strong>render</strong>, which opens file <strong>scene.dat</strong> and reads it 512 bytes at a time, storing the file contents into a heap-allocated block. The first read goes like this:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/readFromPageCache.png" alt="Reading and the page cache"/></p> <p>After 12KB have been read, <tt>render</tt>'s heap and the relevant page frames look thus:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/nonMappedFileRead.png" alt="Non-mapped file read"/></p> <p>This looks innocent enough, but there's a lot going on. First, even though this program uses regular <tt>read</tt> calls, three 4KB page frames are now in the page cache storing part of <tt>scene.dat</tt>. People are sometimes surprised by this, but <strong>all regular file I/O happens through the page cache</strong>. In x86 Linux, the kernel thinks of a file as a sequence of 4KB chunks. If you read a single byte from a file, the whole 4KB chunk containing the byte you asked for is read from disk and placed into the page cache. This makes sense because sustained disk throughput is pretty good and programs normally read more than just a few bytes from a file region. The page cache knows the position of each 4KB chunk within the file, depicted above as #0, #1, etc. Windows uses 256KB <strong>views</strong> analogous to pages in the Linux page cache.</p> <p>Sadly, in a regular file read the kernel must copy the contents of the page cache into a user buffer, which not only takes cpu time and hurts the <a href="http://duartes.org/gustavo/blog/intel-cpu-caches">cpu caches</a>, but also <strong>wastes physical memory with duplicate data</strong>. As per the diagram above, the <tt>scene.dat</tt> contents are stored twice, and each instance of the program would store the contents an additional time. We've mitigated the disk latency problem but failed miserably at everything else. <strong>Memory-mapped files</strong> are the way out of this madness:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/mappedFileRead.png" alt="Mapped file read"/></p> <p>When you use file mapping, the kernel maps your program's virtual pages directly onto the page cache. This can deliver a significant performance boost: <a href="http://www.amazon.com/Windows-Programming-Addison-Wesley-Microsoft-Technology/dp/0321256190/">Windows System Programming</a> reports run time improvements of 30% and up relative to regular file reads, while similar figures are reported for Linux and Solaris in <a href="http://www.amazon.com/Programming-Environment-Addison-Wesley-Professional-Computing/dp/0321525949/">Advanced Programming in the Unix Environment</a>. You might also save large amounts of physical memory, depending on the nature of your application.</p> <p>As always with performance, <a href="http://duartes.org/gustavo/blog/performance-is-a-science">measurement is everything</a>, but memory mapping earns its keep in a programmer's toolbox. The API is pretty nice too, it allows you to access a file as bytes in memory and does not require your soul and code readability in exchange for its benefits. Mind your <a href="http://duartes.org/gustavo/blog/anatomy-of-a-program-in-memory">address space</a> and experiment with <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/mmap.2.html">mmap</a> in Unix-like systems, <a href="http://msdn.microsoft.com/en-us/library/aa366537(VS.85).aspx">CreateFileMapping</a> in Windows, or the many wrappers available in high level languages. When you map a file its contents are not brought into memory all at once, but rather on demand via <a href="http://lxr.linux.no/linux+v2.6.28/mm/memory.c#L2678">page faults</a>. The fault handler <a href="http://lxr.linux.no/linux+v2.6.28/mm/memory.c#L2436">maps your virtual pages</a> onto the page cache after <a href="http://lxr.linux.no/linux+v2.6.28/mm/filemap.c#L1424">obtaining</a> a page frame with the needed file contents. This involves disk I/O if the contents weren't cached to begin with.</p> <p>Now for a pop quiz. Imagine that the last instance of our <tt>render</tt> program exits. Would the pages storing <em>scene.dat</em> in the page cache be freed immediately? People often think so, but that would be a bad idea. When you think about it, it is very common for us to create a file in one program, exit, then use the file in a second program. The page cache must handle that case. When you think <em>more</em> about it, why should the kernel <em>ever</em> get rid of page cache contents? Remember that disk is 5 orders of magnitude slower than RAM, hence a page cache hit is a huge win. So long as there's enough free physical memory, the cache should be kept full. It is therefore <em>not</em> dependent on a particular process, but rather it's a system-wide resource. If you run <tt>render</tt> a week from now and <tt>scene.dat</tt> is still cached, bonus! This is why the kernel cache size climbs steadily until it hits a ceiling. It's not because the OS is garbage and hogs your RAM, it's actually good behavior because in a way free physical memory is a waste. Better use as much of the stuff for caching as possible.</p> <p>Due to the page cache architecture, when a program calls <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/write.2.html">write()</a> bytes are simply copied to the page cache and the page is marked dirty. Disk I/O normally does <strong>not</strong> happen immediately, thus your program doesn't block waiting for the disk. On the downside, if the computer crashes your writes will never make it, hence critical files like database transaction logs must be <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/fsync.2.html">fsync()</a>ed (though one must still worry about drive controller caches, oy!). Reads, on the other hand, normally block your program until the data is available. Kernels employ eager loading to mitigate this problem, an example of which is <strong>read ahead</strong> where the kernel preloads a few pages into the page cache in anticipation of your reads. You can help the kernel tune its eager loading behavior by providing hints on whether you plan to read a file sequentially or randomly (see <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/madvise.2.html">madvise()</a>, <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/readahead.2.html">readahead()</a>, <a href="http://msdn.microsoft.com/en-us/library/aa363858(VS.85).aspx#caching_behavior">Windows cache hints</a>). Linux <a href="http://lxr.linux.no/linux+v2.6.28/mm/filemap.c#L1424">does read-ahead</a> for memory-mapped files, but I'm not sure about Windows. Finally, it's possible to bypass the page cache using <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/open.2.html">O_DIRECT</a> in Linux or <a href="http://msdn.microsoft.com/en-us/library/cc644950(VS.85).aspx">NO_BUFFERING</a> in Windows, something database software often does.</p> <p>A file mapping may be <strong>private</strong> or <strong>shared</strong>. This refers only to <strong>updates</strong> made to the contents in memory: in a private mapping the updates are not committed to disk or made visible to other processes, whereas in a shared mapping they are. Kernels use the <strong>copy on write</strong> mechanism, enabled by page table entries, to implement private mappings. In the example below, both <tt>render</tt> and another program called <tt>render3d</tt> (am I creative or what?) have mapped <tt>scene.dat</tt> privately. <tt>Render</tt> then writes to its virtual memory area that maps the file:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/copyOnWrite.png" alt="The Copy-On-Write mechanism"/></p> <p>The read-only page table entries shown above do <em>not</em> mean the mapping is read only, they're merely a kernel trick to share physical memory until the last possible moment. You can see how 'private' is a bit of a misnomer until you remember it only applies to updates. A consequence of this design is that a virtual page that maps a file privately sees changes done to the file by other programs <em>as long as the page has only been read from</em>. Once copy-on-write is done, changes by others are no longer seen. This behavior is not guaranteed by the kernel, but it's what you get in x86 and makes sense from an API perspective. By contrast, a shared mapping is simply mapped onto the page cache and that's it. Updates are visible to other processes and end up in the disk. Finally, if the mapping above were read-only, page faults would trigger a segmentation fault instead of copy on write.</p> <p>Dynamically loaded libraries are brought into your program's address space via file mapping. There's nothing magical about it, it's the same private file mapping available to you via regular APIs. Below is an example showing part of the address spaces from two running instances of the file-mapping <tt>render</tt> program, along with physical memory, to tie together many of the concepts we've seen.</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/virtualToPhysicalMapping.png" alt="Mapping virtual memory to physical memory"/></p> <p>This concludes our 3-part series on memory fundamentals. I hope the series was useful and provided you with a good mental model of these OS topics.</p>
 [62 Comments](/comments/page-cache.html)
